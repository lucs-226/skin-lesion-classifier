\section{Methodology}
\label{sec:methodology}

\subsection{Architecture: EfficientNet-B3}
We employ EfficientNet-B3, a state-of-the-art convolutional neural network that balances computational efficiency (approx. 12M parameters) with high feature extraction capability. The architecture utilizes a compound coefficient $\phi$ to uniformly scale width, depth, and resolution. We use the B3 variant optimized for $300 \times 300$ inputs, featuring Mobile Inverted Bottleneck (MBConv) blocks with Swish activation ($f(x) = x \cdot \sigma(x)$) and Squeeze-and-Excitation modules.

\subsection{Training Strategy}
Contrary to standard transfer learning approaches that freeze the backbone, we employ a **Full Fine-Tuning** strategy. The entire network (backbone + classification head) is trainable from the first epoch. This allows the model to adapt its deep semantic features specifically to the dermoscopic domain, rather than relying on generic ImageNet features.
\begin{itemize}
    \item \textbf{Classification Head:} The original top layer is replaced by a Dropout layer ($p=0.3$) followed by a Linear Dense layer mapping to the 7 diagnostic classes.
\end{itemize}

\subsection{Handling Imbalance: Hybrid Approach}
We tackle the severe class imbalance (dominance of 'Nevus') using a dual strategy:
\begin{itemize}
    \item \textbf{Sampling:} We utilize a \textbf{Weighted Random Sampler} during data loading. Sampling probabilities are inverse to class frequencies ($P(x) \propto 1/N_{class}$), ensuring each mini-batch is statistically balanced.
    \item \textbf{Loss Function:} We implement the **Focal Loss** to down-weight easy examples and focus training on hard misclassifications.
    \begin{equation}
      FL(p_t) = - (1 - p_t)^\gamma \log(p_t)
    \end{equation}
    We set the focusing parameter $\gamma = 2.0$, which effectively reduces the loss contribution from well-classified examples (mostly benign nevi) and prevents them from overwhelming the gradient.
\end{itemize}

\subsection{Regularization \& Augmentation}
To prevent overfitting given the full fine-tuning regime:
\begin{itemize}
    \item \textbf{Mixup:} We apply Mixup augmentation ($\alpha=0.4$), which trains the network on convex combinations of pairs of examples and their labels: $\tilde{x} = \lambda x_i + (1-\lambda) x_j$. This enforces smoother decision boundaries.
    \item \textbf{Random Erasing:} Applied with probability $p=0.2$ to simulate occlusion and force robustness.
\end{itemize}
