\section{Methodology}
\label{sec:methodology}

\subsection{Architecture: EfficientNet-B3}
We employ EfficientNet-B3, a state-of-the-art convolutional neural network that balances computational efficiency (approx. 12M parameters) with high feature extraction capability.
\vspace{5pt}

\noindent\textbf{Compound Scaling:} The architecture utilizes a compound coefficient $\phi$ to uniformly scale width, depth, and resolution. We use the B3 variant optimized for $300 \times 300$ inputs, utilizing Mobile Inverted Bottleneck (MBConv) blocks with Swish activation ($f(x) = x \cdot \sigma(x)$) and Squeeze-and-Excitation modules to adaptively recalibrate channel-wise feature responses.

\subsection{Handling Class Imbalance}
The HAM10000 dataset is highly imbalanced, with the 'Nevus' class dominating over rare classes like 'Dermatofibroma'. Instead of modifying the loss function, we intervene at the data loading stage using a **Weighted Random Sampler**.
This strategy assigns a sampling probability to each image inversely proportional to its class frequency: $P(x_i) \propto 1/N_{class}$. Consequently, each mini-batch contains a balanced distribution of classes, ensuring that the gradient updates are not biased towards the majority class and preventing the model from converging to trivial solutions.

% FIXED LINE BELOW (Added backslash before &)
\subsection{Advanced Regularization \& Augmentation}
To prevent overfitting and improve generalization on the decision boundaries, we implement a robust augmentation pipeline.

\vspace{5pt}
\noindent\textbf{Mixup:} We employ Mixup, a data-agnostic data augmentation routine. It constructs virtual training examples by computing a convex combination of pairs of inputs and their labels:
\begin{equation}
  \tilde{x} = \lambda x_i + (1-\lambda) x_j, \quad \tilde{y} = \lambda y_i + (1-\lambda) y_j
  \label{eq:mixup}
\end{equation}
where $\lambda \sim Beta(\alpha, \alpha)$. This encourages the model to behave linearly in-between training examples, smoothing the decision landscape.

\vspace{5pt}
\noindent\textbf{Random Erasing:} To simulate occlusion and force the network to learn robust features (rather than relying on specific local artifacts), we apply Random Erasing. This technique randomly selects a rectangular region $R_e$ in the image and replaces its pixels with random values.

\subsection{Transfer Learning Strategy}
The backbone is initialized with ImageNet weights and frozen during the initial epochs to stabilize the custom classification head (Global Average Pooling + Dropout $p=0.2$ + Dense Softmax).
