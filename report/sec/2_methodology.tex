\section{Methodology}
\label{sec:methodology}

\subsection{Experimental Protocol: Stratified K-Fold}
To ensure statistically robust performance estimates and prevent overfitting to a specific data split, we employed \textbf{Stratified K-Fold Cross-Validation} ($K=5$). Unlike standard random splitting, this strategy partitions the dataset into $K$ disjoint folds while strictly preserving the original class distribution ratios in each subset. The model is trained $K$ times, using $K-1$ folds for training and the remaining fold for validation. This rigorous validation protocol ensures that our metrics reflect the model's generalization capability across the entire data distribution, reducing the bias introduced by the severe class imbalance of HAM10000.

\subsection{Architecture and Training Strategy}
We employ \textbf{EfficientNet-B3}, selected for its efficiency (12M parameters) and Compound Scaling capabilities ($300 \times 300$ input). Contrary to conservative approaches, we adopt a \textbf{Full Fine-Tuning} strategy: the entire backbone is trainable from the first epoch, allowing the deep MBConv blocks to adapt their spatial filters specifically to dermoscopic textures rather than generic ImageNet features.
The training utilizes the \textbf{AdamW} optimizer ($3r=3\times10^{-4}$) with a \textbf{Cosine Annealing} scheduler to foster convergence into flat, robust minima.

\subsection{Augmentation and Regularization}
To improve robustness against acquisition variability, we implemented a strong augmentation pipeline. Images undergo geometric transformations (random flips, $20^{\circ}$ rotations) and photometric distortions via \texttt{ColorJitter}. We further integrated \textbf{Random Erasing} ($p=0.2$) to simulate occlusions and \textbf{Mixup} ($\alpha=0.4$) to enforce smoother decision boundaries.

\subsection{Imbalance Mitigation: Custom Focal Loss}
We address the long-tail distribution via a custom \textbf{Focal Loss} that down-weights easy negatives. Defined as $FL(p_t) = \alpha (1 - p_t)^\gamma \cdot CE(p, y)$ with $\gamma=2$, it effectively suppresses the gradient contribution from the majority 'Nevus' class ($p_t \to 1$), forcing the model to focus learning on hard, misclassified malignant lesions. This is coupled with a \textbf{Weighted Random Sampler} to balance mini-batch statistics.
