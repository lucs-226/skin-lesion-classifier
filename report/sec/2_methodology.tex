\section{Methodology}
\label{sec:methodology}

\subsection{Architecture: EfficientNet-B3}
We employ EfficientNet-B3, selected for its optimal trade-off between parameter efficiency (approx. 12M parameters) and accuracy. The network scales the baseline B0 depth by a factor of $\alpha=1.2$ and width by $\beta=1.1$.

\subsection{Transfer Learning Strategy}
\begin{itemize}
    \item \textbf{Input Processing:} Images are resized to $300 \times 300$ pixels. The B3 backbone expects inputs in range $[0, 255]$ and handles internal normalization.
    \item \textbf{Feature Extractor:} The convolutional base is frozen to retain ImageNet features.
    \item \textbf{Classification Head:} We append a Global Average Pooling layer, followed by a Dropout layer ($p=0.2$) to reduce overfitting, and a final Dense layer with Softmax activation for the 7 classes.
\end{itemize}

\subsection{Handling Imbalance}
Given the prevalence of the 'Nv' (Nevus) class, we compute class weights $W_j = N_{total} / (N_{classes} \cdot N_j)$ and apply them to the Categorical Cross-Entropy loss function:
\begin{equation}
  L = - \sum_{i=1}^{N} \sum_{c=1}^{C} w_c \cdot y_{i,c} \cdot \log(\hat{y}_{i,c})
  \label{eq:weighted_loss}
\end{equation}
