\section{Methodology}
\label{sec:methodology}

\subsection{Architecture: EfficientNet-B3}
We employ EfficientNet-B3, selected for its optimal trade-off between parameter efficiency (approx. 12M parameters) and feature extraction capability. The network utilizes compound scaling to uniformly scale network width, depth, and resolution ($300 \times 300$ pixels), ensuring that the receptive fields are sufficiently large to capture dermatological patterns.

\subsection{Training Strategy}
Contrary to standard transfer learning approaches that often freeze the convolutional backbone, we employ a \textbf{Full Fine-Tuning} strategy. The entire EfficientNet-B3 architecture is trainable from the first epoch. We initialize weights from an ImageNet pre-trained checkpoint and allow gradients to update all parameters. This enables the deep semantic features of the MBConv blocks to adapt specifically to dermoscopic textures rather than relying on generic edge detectors.

\subsection{Advanced Augmentation Pipeline}
To guarantee robustness and prevent overfitting, we implemented an heavy augmentation strategy:
\begin{itemize}
    \item \textbf{Geometric and Photometric:} Images undergo random horizontal and vertical flips, rotations ($20^{\circ}$), and \texttt{ColorJitter} to simulate varying lighting conditions.
    \item \textbf{Random Erasing:} Applied with probability $p=0.2$. This technique randomly masks rectangular regions of the image, simulating occlusions (e.g., hair, artifacts) and forcing the network to learn robust features across the entire lesion rather than relying on a single discriminative part.
    \item \textbf{Mixup Regularization:} We employ Mixup ($\alpha=0.4$), which trains the network on convex combinations of pairs of examples and their labels: $\tilde{x} = \lambda x_i + (1-\lambda) x_j$. This encourages the model to learn smoother decision boundaries between classes, reducing overconfidence on ambiguous samples.
\end{itemize}

\subsection{Imbalance Mitigation: Hybrid Approach}
The HAM10000 dataset exhibits a severe long-tail distribution. We tackle this using a dual strategy:

\textbf{1. Weighted Random Sampler:} During data loading, we oversample the minority classes to ensure that each mini-batch contains a roughly uniform distribution of labels. This prevents the Batch Normalization layers from biasing their statistics towards the majority class ('Nevus').

\textbf{2. Custom Focal Loss:} We implement a loss function that down-weights easy negatives. Let $p_t = \exp(-CE(p, y))$; our objective is:
\begin{equation}
    FL(p_t) = \alpha (1 - p_t)^\gamma \cdot CE(p, y)
    \label{eq:focal_loss}
\end{equation}
With $\gamma=2$, the term $(1 - p_t)^\gamma$ effectively zeros out the loss for well-classified examples, focusing the gradient updates on hard, misclassified lesions.
