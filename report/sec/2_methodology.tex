\section{Methodology}
\label{sec:methodology}

\subsection{Architecture: EfficientNet-B3}
We employ EfficientNet-B3, selected for its optimal trade-off between parameter efficiency (approx. 12M parameters) and feature extraction capability. The network utilizes compound scaling to uniformly scale network width, depth, and resolution ($300 \times 300$ pixels), ensuring that the receptive fields are sufficiently large to capture dermatological patterns.

\subsection{Training Strategy}
Contrary to standard transfer learning approaches that often freeze the convolutional backbone, we employ a \textbf{Full Fine-Tuning} strategy. The entire EfficientNet-B3 architecture is trainable from the first epoch. We initialize weights from an ImageNet pre-trained checkpoint and allow gradients to update all parameters. This enables the deep semantic features of the MBConv blocks to adapt specifically to dermoscopic textures (e.g., pigment networks, globules) rather than relying on generic edge detectors.

To guarantee robustness against variations in lighting and acquisition angles, we implemented a rigorous data augmentation pipeline. During training, images undergo:
\begin{itemize}
    \item \textbf{Geometric Transformations:} Random horizontal and vertical flips, coupled with random rotations (up to $20^{\circ}$).
    \item \textbf{Photometric Distortions:} Application of \texttt{ColorJitter} to randomly perturb brightness, contrast, and saturation.
\end{itemize}

\subsection{Loss Function: Focal Loss}
The HAM10000 dataset is characterized by a severe long-tail distribution, where the 'Nevus' class dominates over rare malignancies. In standard training with Cross-Entropy (CE) loss, the accumulation of gradients from frequent, easy-to-classify background examples can overwhelm the training signal from hard, rare examples.

To address this, we implement a custom \textbf{Focal Loss}. Let $p_t$ be the model's estimated probability for the target class, derived from the computed Cross-Entropy term as $p_t = \exp(-CE(p, y))$. Our loss function is formulated as:

\begin{equation}
    FL(p_t) = \alpha (1 - p_t)^\gamma \cdot CE(p, y)
    \label{eq:focal_loss}
\end{equation}

where $\gamma$ is the focusing parameter and $\alpha$ is the balancing factor.
\begin{itemize}
    \item \textbf{Focusing Mechanism ($\gamma=2$):} The term $(1 - p_t)^\gamma$ acts as a dynamic scale. When an example is misclassified ($p_t \to 0$), the factor is near 1. Conversely, as $p_t \to 1$ (well-classified), the factor approaches 0. This effectively "down-weights" easy examples (like common Nevi), forcing the optimizer to focus on hard, ambiguous lesions.
    \item \textbf{Implementation:} We set \texttt{reduction='mean'} to normalize the loss across the batch, ensuring stable gradient updates for the fine-tuned backbone.
\end{itemize}
